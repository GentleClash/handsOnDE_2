x-airflow-common:
  &airflow-common
  environment:
    &airflow-environment
    AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
    AIRFLOW__CORE__LOAD_EXAMPLES: False
    
    AIRFLOW__LOGGING__REMOTE_LOGGING: False
    AIRFLOW__LOGGING__LOGGING_LEVEL: INFO

    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: http://airflow-api-server:8080/execution/

    AIRFLOW__CORE__SECRET_KEY: 97935b51f390ce53c05e19d8e725d1a7c6722af0a68a7599e1ae2dc4571cf7a821a417eaf2e5d2a62e4154764a18d9cbf3004d9fde61297b80b6936ed83edaa5
    AIRFLOW__API_AUTH__JWT_SECRET: 97935b51f390ce53c05e19d8e725d1a7c6722af0a68a7599e1ae2dc4571cf7a821a417eaf2e5d2a62e4154764a18d9cbf3004d9fde61297b80b6936ed83edaa5

    


services:
  broker_1:
    image: apache/kafka:latest
    container_name: broker_1
    ports:
      - "9092:9092"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@broker_1:29093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,CONTROLLER://0.0.0.0:29093,EXTERNAL://0.0.0.0:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker_1:29092,EXTERNAL://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 2
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: 3
    healthcheck:
      test: ["CMD", "/opt/kafka/bin/kafka-broker-api-versions.sh", "--bootstrap-server", "localhost:29092"]
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 10s

  broker_2:
    image: apache/kafka:latest
    container_name: broker_2
    ports:
      - "9093:9092"
    environment:
      KAFKA_NODE_ID: 2
      KAFKA_PROCESS_ROLES: broker
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@broker_1:29093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,EXTERNAL://0.0.0.0:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker_2:29092,EXTERNAL://localhost:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 2
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: 3
    healthcheck:
      test: ["CMD", "/opt/kafka/bin/kafka-broker-api-versions.sh", "--bootstrap-server", "localhost:29092"]
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 10s
  
  producer:
    build:
      context: .
      dockerfile: Dockerfile.app 
    container_name: kafka_producer
    depends_on:
      broker_1:
        condition: service_healthy
      broker_2:
        condition: service_healthy
    environment:
      KAFKA_PRODUCER_BOOTSTRAP_SERVERS: broker_1:29092,broker_2:29092
      SYNTHETIC_DATA_OUTPUT_DIR: /data
    volumes:
      - ./data:/data
    command: ["python", "producer/run_stream.py"]

  consumer_1:
    build:
      context: .
      dockerfile: Dockerfile.app
    container_name: kafka_consumer_1
    depends_on:
      broker_1:
        condition: service_healthy
      broker_2:
        condition: service_healthy
    environment:
      KAFKA_CONSUMER_BOOTSTRAP_SERVERS: broker_1:29092,broker_2:29092
      KAFKA_CONSUMER_GROUP: raw-data-archive
      KAFKA_CONSUMER_TOPICS: orders.events,riders.events
      KAFKA_CONSUMER_AUTO_OFFSET_RESET: earliest
      DATA_LAKE_BASE_DIR: /data_lake/raw
    volumes:
      - ./data_lake:/data_lake
    command: ["python", "consumers/kafka_to_lake.py"]

  consumer_2:
    build:
      context: .
      dockerfile: Dockerfile.app
    container_name: kafka_consumer_2
    depends_on:
      broker_1:
        condition: service_healthy
      broker_2:
        condition: service_healthy
    environment:
      KAFKA_CONSUMER_BOOTSTRAP_SERVERS: broker_1:29092,broker_2:29092
      KAFKA_CONSUMER_GROUP: raw-data-archive
      KAFKA_CONSUMER_TOPICS: orders.events,riders.events
      KAFKA_CONSUMER_AUTO_OFFSET_RESET: earliest
      DATA_LAKE_BASE_DIR: /data_lake/raw
    volumes:
      - ./data_lake:/data_lake
    command: ["python", "consumers/kafka_to_lake.py"]

  
  airflow-db:
    image: postgres:17.7-alpine
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s
    volumes:
      - airflow-db-volume:/var/lib/postgresql/data
  
  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow_init
    depends_on:
      airflow-db:
        condition: service_healthy
    environment:
      <<: *airflow-environment
      _AIRFLOW_DB_MIGRATE: true
      _AIRFLOW_WWW_USER_CREATE: true
      _AIRFLOW_WWW_USER_USERNAME: admin
      _AIRFLOW_WWW_USER_PASSWORD: admin
    command: version
  
  airflow-api-server:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow_api_server
    hostname: airflow-api-server
    restart: always
    depends_on:
      airflow-db:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
      airflow-dag-processor:
        condition: service_healthy
    ports:
      - "8080:8080"  
    environment:
      <<: *airflow-environment
      AIRFLOW__API__PORT: 8080
    volumes:
      - ./logs:/opt/airflow/logs
      - ./dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
      - ./data_lake:/opt/airflow/data_lake
    command: api-server
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/docs || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s

  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow_scheduler
    hostname: airflow-scheduler
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      airflow-db:
        condition: service_healthy
      airflow-api-server:  
        condition: service_healthy
    environment:
      <<: *airflow-environment
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__DAG_PROCESSOR__REFRESH_INTERVAL: 10
      KAFKA_BOOTSTRAP_SERVERS: broker_1:29092,broker_2:29092 

    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $${HOSTNAME}"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    volumes:
      - ./logs:/opt/airflow/logs
      - ./dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
      - ./data_lake:/opt/airflow/data_lake
    command: scheduler

  airflow-dag-processor:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow_dag_processor
    hostname: airflow-dag-processor
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      airflow-db:
        condition: service_healthy
    environment:
      <<: *airflow-environment
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__CORE__INTERNAL_API_URL: http://airflow-api-server:8080
      AIRFLOW__DAG_PROCESSOR__REFRESH_INTERVAL: 10
      
    volumes:
      - ./logs:/opt/airflow/logs
      - ./dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
      - ./data_lake:/opt/airflow/data_lake
    command: dag-processor
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type DagProcessorJob --hostname $${HOSTNAME}"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

volumes:
  airflow-db-volume: